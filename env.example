# LLM Service Configuration
VITE_LLM_PROVIDER=openai
VITE_LLM_API_KEY=your-api-key-here
VITE_LLM_ENDPOINT=https://api.openai.com/v1
VITE_LLM_MODEL=gpt-4o-mini
VITE_LLM_MAX_TOKENS=2000
VITE_LLM_TEMPERATURE=0.7

# Translation Service Configuration
VITE_ENABLE_MOCK_TRANSLATION=true

# Alternative providers (comment out unused ones)
# VITE_LLM_PROVIDER=anthropic
# VITE_LLM_API_KEY=your-claude-api-key
# VITE_LLM_ENDPOINT=https://api.anthropic.com/v1
# VITE_LLM_MODEL=claude-3-haiku-20240307

# VITE_LLM_PROVIDER=gemini
# VITE_LLM_API_KEY=your-gemini-api-key
# VITE_LLM_ENDPOINT=https://generativelanguage.googleapis.com/v1beta
# VITE_LLM_MODEL=gemini-1.5-flash
# VITE_GEMINI_PROJECT_ID=your-project-id # Optional

# VITE_LLM_PROVIDER=llama
# VITE_LLM_API_KEY=your-llama-api-key-or-none-for-ollama
# VITE_LLM_ENDPOINT=http://localhost:11434 # Ollama default
# VITE_LLM_MODEL=llama3.1:8b
# VITE_LLAMA_PROVIDER=ollama # ollama, groq, together, replicate, custom
# VITE_LLAMA_SYSTEM_PROMPT=You are a helpful assistant.
# VITE_LLAMA_STOP_SEQUENCES=["<|end|>", "<|stop|>"]
# VITE_LLAMA_HEADERS={"X-Custom-Header": "value"}

# VITE_LLM_PROVIDER=custom
# VITE_LLM_API_KEY=your-custom-api-key
# VITE_LLM_ENDPOINT=https://your-custom-endpoint.com/v1
# VITE_LLM_MODEL=your-custom-model 